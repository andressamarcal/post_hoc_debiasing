{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Magics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import shutil\n",
    "import re\n",
    "import json\n",
    "from pathlib import Path\n",
    "import os\n",
    "from collections import namedtuple\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "plt.style.use('ggplot')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "input('are you sure?')\n",
    "machines = \"\"\"\n",
    "\"\"\".split('\\n')[1:-1]\n",
    "\n",
    "\n",
    "model_path = Path('nas_results_base/')\n",
    "model_path.mkdir(exist_ok=True)\n",
    "for i in range(len(machines)):\n",
    "    (model_path/str(i)).mkdir(exist_ok=True)\n",
    "\n",
    "location = '~/fairness/deco/src/deco/post_hoc/results'\n",
    "\n",
    "for i, machine in enumerate(machines):\n",
    "    os.system(f\"scp  -i ~/.ssh/ec2-key \\\"ubuntu@{machine}:{location}/*\\\" {str(model_path)}/{i}/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = Path('nas_results_bm/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list(model_path.glob('**/*test_output.json')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, re\n",
    "\n",
    "data = {}\n",
    "for file_results in model_path.glob('**/*test_output.json'):\n",
    "    keys = re.match(fr'{model_path}/(\\d)/(\\w+_\\w+_\\d)_(\\d)_baselines_test_output.json', str(file_results))\n",
    "    with open(file_results) as fh:\n",
    "        datum = json.load(fh)\n",
    "    data.update({keys.groups() : {(k,kk): vv for k,v in datum.items() for kk,vv in v.items()}})\n",
    "    \n",
    "plotdf = pd.DataFrame(data).T.unstack(0).unstack(0).describe().loc[['count', 'mean', 'std']].T.unstack(1).reorder_levels([2,1,0]).sort_index().swaplevel(0,1,1).sort_index(1)\n",
    "plotdf = plotdf['objective'].droplevel(0).unstack(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotdf.index = ['base', 'more dropout', 'more width', 'more layers']\n",
    "\n",
    "# titlename = {'spd': 'Statistical Parity Difference', 'eod': 'Equal Opportunity Difference', 'aod': 'Average Odds Difference'}\n",
    "column_order = ['default', 'ROC', 'EqOdds', 'CalibEqOdds', 'Random', 'adversarial', 'layerwiseOpt']\n",
    "col_rename = {'default': 'Default', 'adversarial': 'Adversarial', 'layerwiseOpt': 'LayerwiseOpt'}\n",
    "\n",
    "plotmean = plotdf['mean'].reindex(columns=column_order).rename(columns=col_rename)\n",
    "plotstd = plotdf['std'].reindex(columns=column_order).rename(columns=col_rename)\n",
    "plotmean.plot(kind='bar', yerr=plotstd, figsize=(10,5), rot=0)\n",
    "plt.ylabel('Objective: $\\lambda$|SPD| + $(1-\\lambda)(1-$accuracy$)$')\n",
    "plt.xlabel('Architecture BM (sex)')\n",
    "plt.gca().legend(loc='upper center', bbox_to_anchor=(0.5, -0.15), ncol=7)\n",
    "plt.tight_layout()\n",
    "# plt.savefig(f'images/{metric}_results.pdf')\n",
    "plt.savefig(f'images/multinet_bm_results.png')\n",
    "plt.savefig(f'images/multinet_bm_results.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from post_hoc.posthoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = ['adult', 'bank', 'compas']\n",
    "results = {}\n",
    "\n",
    "for dataset in datasets:\n",
    "    train, valid, test, priv, unpriv = get_data(dataset, 1)\n",
    "    priv_index = train.protected_attribute_names.index(list(priv[0].keys())[0])\n",
    "\n",
    "    scale_orig = StandardScaler()\n",
    "    X_train = torch.tensor(scale_orig.fit_transform(train.features), dtype=torch.float32)\n",
    "    y_train = torch.tensor(train.labels.ravel(), dtype=torch.float32)\n",
    "    # p_train = train.protected_attributes[:, priv_index]\n",
    "\n",
    "    X_valid = torch.tensor(scale_orig.transform(valid.features), dtype=torch.float32)\n",
    "    y_valid = torch.tensor(valid.labels.ravel(), dtype=torch.float32)\n",
    "    p_valid = valid.protected_attributes[:, priv_index]\n",
    "\n",
    "    X_test = torch.tensor(scale_orig.transform(test.features), dtype=torch.float32)\n",
    "    y_test = torch.tensor(test.labels.ravel(), dtype=torch.float32)\n",
    "    p_test = test.protected_attributes[:, priv_index]\n",
    "\n",
    "    model = Model(X_train.size(1))\n",
    "\n",
    "    roc_auc_scores = []\n",
    "    accs = []\n",
    "    for path in model_path.glob('*'):\n",
    "        if dataset in str(path):\n",
    "            model.load_state_dict(torch.load(path))\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                yhat_test = model(X_test)[:, 0].reshape(-1, 1).numpy()\n",
    "            roc_auc_scores.append(roc_auc_score(y_test, yhat_test))\n",
    "\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                yhat_valid = model(X_valid)[:, 0].reshape(-1, 1).numpy()\n",
    "                \n",
    "            threshs = np.linspace(0,1,1001)\n",
    "            bthresh = threshs[np.argmax([accuracy_score(y_valid, yhat_valid > thresh) for thresh in threshs])]\n",
    "            accs.append(accuracy_score(y_test, yhat_test > bthresh))\n",
    "    results[(dataset,  'neural network', 'roc_auc')] = roc_auc_scores\n",
    "    results[(dataset, 'neural network', 'accuracy')] = accs\n",
    "    \n",
    "    roc_auc_scores = []\n",
    "    accs = []\n",
    "    for i in range(10):\n",
    "        lr = LogisticRegressionCV()\n",
    "        lr.fit(X_train, y_train)\n",
    "        \n",
    "        yhat_test = lr.predict_proba(X_test)[:,1]\n",
    "        roc_auc_scores.append(roc_auc_score(y_test, yhat_test))\n",
    "        \n",
    "        yhat_valid = lr.predict_proba(X_valid)[:,1]\n",
    "        threshs = np.linspace(0,1,1001)\n",
    "        bthresh = threshs[np.argmax([accuracy_score(y_valid, yhat_valid > thresh) for thresh in threshs])]\n",
    "        accs.append(accuracy_score(y_test, yhat_test > bthresh))\n",
    "    results[(dataset,  'logistic regression', 'roc_auc')] = roc_auc_scores\n",
    "    results[(dataset, 'logistic regression', 'accuracy')] = accs\n",
    "    \n",
    "    \n",
    "    roc_auc_scores = []\n",
    "    accs = []\n",
    "    for i in range(10):\n",
    "        rf = RandomForestClassifier()\n",
    "        rf.fit(X_train, y_train)\n",
    "        \n",
    "        yhat_test = rf.predict_proba(X_test)[:,1]\n",
    "        roc_auc_scores.append(roc_auc_score(y_test, yhat_test))\n",
    "        \n",
    "        yhat_valid = rf.predict_proba(X_valid)[:,1]\n",
    "        threshs = np.linspace(0,1,1001)\n",
    "        bthresh = threshs[np.argmax([accuracy_score(y_valid, yhat_valid > thresh) for thresh in threshs])]\n",
    "        accs.append(accuracy_score(y_test, yhat_test > bthresh))\n",
    "    results[(dataset,  'random forest', 'roc_auc')] = roc_auc_scores\n",
    "    results[(dataset, 'random forest', 'accuracy')] = accs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = pd.DataFrame(results).describe().loc[['count', 'mean', 'std']].T\n",
    "df = (tmp['mean'].map('{:.3f}'.format) + ' $\\pm$ ' + tmp['std'].map('{:.3f}'.format)).unstack(1)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.to_latex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, valid, test, priv, unpriv = get_data('adult', 1)\n",
    "priv_index = train.protected_attribute_names.index(list(priv[0].keys())[0])\n",
    "\n",
    "scale_orig = StandardScaler()\n",
    "X_train = torch.tensor(scale_orig.fit_transform(train.features), dtype=torch.float32)\n",
    "y_train = torch.tensor(train.labels.ravel(), dtype=torch.float32)\n",
    "# p_train = train.protected_attributes[:, priv_index]\n",
    "\n",
    "X_valid = torch.tensor(scale_orig.transform(valid.features), dtype=torch.float32)\n",
    "y_valid = torch.tensor(valid.labels.ravel(), dtype=torch.float32)\n",
    "p_valid = valid.protected_attributes[:, priv_index]\n",
    "\n",
    "X_test = torch.tensor(scale_orig.transform(test.features), dtype=torch.float32)\n",
    "y_test = torch.tensor(test.labels.ravel(), dtype=torch.float32)\n",
    "p_test = test.protected_attributes[:, priv_index]\n",
    "\n",
    "model = Model(X_train.size(1))\n",
    "deltas = []\n",
    "biases = []\n",
    "for path in model_path.glob('*'):\n",
    "    if 'adult' in str(path):\n",
    "        deltas.append([])\n",
    "        biases.append([])\n",
    "        for i in range(1000):\n",
    "            model.load_state_dict(torch.load(path))\n",
    "            delta = []\n",
    "            for param in model.parameters():\n",
    "                delta.append(torch.randn_like(param) * 0.1 + 1)\n",
    "                param.data = param.data * delta[-1]\n",
    "            delta = torch.cat([x.reshape(-1) for x in delta])\n",
    "            deltas[-1].append(delta)\n",
    "\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                yhat_test = model(X_test)[:, 0].reshape(-1, 1).numpy()\n",
    "            bias = compute_bias(yhat_test, y_test.numpy(), p_test, metric='spd')\n",
    "            biases[-1].append(bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape = len(deltas[0])\n",
    "train = torch.randperm(shape)[:int(shape*0.8)]\n",
    "test = torch.randperm(shape)[int(shape*0.8):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "coefs = []\n",
    "scores = []\n",
    "for d, b in zip(deltas, biases):\n",
    "    lr = LinearRegression()\n",
    "    lr.fit(torch.stack(d).numpy()[train], np.array(b)[train])\n",
    "    scores.append(lr.score(torch.stack(d).numpy()[test], np.array(b)[test]))\n",
    "    coefs.append(lr.coef_)\n",
    "coefs = np.array(coefs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'{np.array(scores).mean():.3f} $\\pm$ {2.2*np.array(scores).std():.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "pdf = pd.DataFrame(np.sort(np.abs(coefs)))\n",
    "pdf.mean().plot()\n",
    "plt.fill_between(pdf.std().index, pdf.mean() - 2.2*pdf.std(), pdf.mean() + 2.2*pdf.std(),alpha=0.4)\n",
    "plt.xlabel('index of sorted coefficients')\n",
    "plt.ylabel('coefficient value')\n",
    "plt.savefig('coefs_sort.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.linalg import svd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U,s,Vh = svd(normalize(coefs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U.shape, s.shape, Vh.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(np.arange(0,s.size), s)\n",
    "plt.xlabel('singular value index')\n",
    "plt.ylabel('singular value')\n",
    "plt.savefig('coefs_svd.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yash1 = 'ec2-3-233-221-255.compute-1.amazonaws.com'\n",
    "yash2 = 'ec2-3-230-154-171.compute-1.amazonaws.com'\n",
    "yash3 = 'ec2-3-235-41-184.compute-1.amazonaws.com'\n",
    "yash4 = 'ec2-3-223-3-236.compute-1.amazonaws.com'\n",
    "yashs = [yash1, yash2, yash3, yash4]\n",
    "\n",
    "results_path = Path('NAS/')\n",
    "# shutil.rmtree(results_path)\n",
    "results_path.mkdir(exist_ok=True)\n",
    "\n",
    "location = '~/fairness/deco/src/deco/post_hoc/results'\n",
    "\n",
    "for i, yash in enumerate(yashs):\n",
    "    results_dirpath = Path(f'NAS/{i}')\n",
    "    results_dirpath.mkdir(exist_ok=True)\n",
    "    os.system(f\"scp  -i ~/.ssh/ec2-key \\\"ubuntu@{yash}:{location}/*\\\" {str(results_dirpath)}/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, re\n",
    "data = {}\n",
    "for result_file in results_path.glob('**/*.json'):\n",
    "#     print(result_file)\n",
    "    matches = re.match(r'NAS/(?P<nn_type>\\d+)/(?P<dataset>\\w+)_(?P<bias>\\w+)_(?P<protected>\\d+)_(?P<iter>\\d+)_baselines_(?P<fold>\\w+)_output.json', str(result_file))\n",
    "    if matches.group('fold') == 'valid':\n",
    "        continue\n",
    "    with open(result_file, 'r') as fh:\n",
    "        datum = json.load(fh)\n",
    "    datum = {(matches.group('nn_type'), matches.group('iter'), k): v for k,v in datum.items()}\n",
    "    data.update(datum)\n",
    "#     data = dict(data, **datum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotdf = pd.DataFrame(data).stack(1).unstack(0).describe().loc[['count', 'mean', 'std']].T.unstack(-1).loc[:,(slice(None, None),'objective')].droplevel(1,1).unstack(1)\n",
    "plotdf.index = ['neural network 1', 'neural network 2', 'neural network 3', 'neural network 4']\n",
    "\n",
    "# titlename = {'spd': 'Statistical Parity Difference', 'eod': 'Equal Opportunity Difference', 'aod': 'Average Odds Difference'}\n",
    "column_order = ['default', 'ROC', 'EqOdds', 'CalibEqOdds', 'Random', 'adversarial', 'layerwiseOpt']\n",
    "col_rename = {'default': 'Default', 'adversarial': 'Adversarial', 'layerwiseOpt': 'LayerwiseOpt'}\n",
    "\n",
    "plotmean = plotdf['mean'].reindex(columns=column_order).rename(columns=col_rename)\n",
    "plotstd = plotdf['std'].reindex(columns=column_order).rename(columns=col_rename)\n",
    "plotmean.plot(kind='bar', yerr=plotstd, figsize=(10,5), rot=0)\n",
    "plt.ylabel('Objective')\n",
    "plt.xlabel('Dataset')\n",
    "plt.gca().legend(loc='upper center', bbox_to_anchor=(0.5, -0.15), ncol=7)\n",
    "plt.tight_layout()\n",
    "# plt.savefig(f'images/{metric}_results.pdf')\n",
    "plt.savefig(f'images/multinet_results.png')\n",
    "plt.savefig(f'images/multinet_results.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deco",
   "language": "python",
   "name": "deco"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
