{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Magics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, f1_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aif360.datasets import AdultDataset, GermanDataset, CompasDataset, BankDataset\n",
    "from aif360.metrics import BinaryLabelDatasetMetric\n",
    "from aif360.metrics import ClassificationMetric\n",
    "from aif360.metrics.utils import compute_boolean_conditioning_vector\n",
    "\n",
    "from aif360.algorithms.preprocessing.optim_preproc_helpers.data_preproc_functions\\\n",
    "                import load_preproc_data_adult, load_preproc_data_compas\n",
    "\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## import dataset\n",
    "dataset_used = \"adult\" # \"adult\", \"german\", \"compas\"\n",
    "protected_attribute_used = 1 # 1, 2\n",
    "\n",
    "if dataset_used == \"adult\":\n",
    "    dataset_orig = AdultDataset()\n",
    "#     dataset_orig = load_preproc_data_adult()\n",
    "    if protected_attribute_used == 1:\n",
    "        privileged_groups = [{'sex': 1}]\n",
    "        unprivileged_groups = [{'sex': 0}]\n",
    "    else:\n",
    "        privileged_groups = [{'race': 1}]\n",
    "        unprivileged_groups = [{'race': 0}]\n",
    "    \n",
    "elif dataset_used == \"german\":\n",
    "    dataset_orig = GermanDataset()\n",
    "    if protected_attribute_used == 1:\n",
    "        privileged_groups = [{'sex': 1}]\n",
    "        unprivileged_groups = [{'sex': 0}]\n",
    "    else:\n",
    "        privileged_groups = [{'age': 1}]\n",
    "        unprivileged_groups = [{'age': 0}]\n",
    "    \n",
    "elif dataset_used == \"compas\":\n",
    "#     dataset_orig = CompasDataset()\n",
    "    dataset_orig = load_preproc_data_compas()\n",
    "    if protected_attribute_used == 1:\n",
    "        privileged_groups = [{'sex': 1}]\n",
    "        unprivileged_groups = [{'sex': 0}]\n",
    "    else:\n",
    "        privileged_groups = [{'race': 1}]\n",
    "        unprivileged_groups = [{'race': 0}]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_orig_train, dataset_orig_vt = dataset_orig.split([0.6], shuffle=True)\n",
    "dataset_orig_valid, dataset_orig_test = dataset_orig_vt.split([0.5], shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print out some labels, names, etc.\n",
    "display(Markdown(\"#### Dataset shape\"))\n",
    "print(dataset_orig_train.features.shape)\n",
    "display(Markdown(\"#### Favorable and unfavorable labels\"))\n",
    "print(dataset_orig_train.favorable_label, dataset_orig_train.unfavorable_label)\n",
    "display(Markdown(\"#### Protected attribute names\"))\n",
    "print(dataset_orig_train.protected_attribute_names)\n",
    "display(Markdown(\"#### Privileged and unprivileged protected attribute values\"))\n",
    "print(dataset_orig_train.privileged_protected_attributes, dataset_orig_train.unprivileged_protected_attributes)\n",
    "display(Markdown(\"#### Dataset feature names\"))\n",
    "print(dataset_orig_train.feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scale_orig = StandardScaler()\n",
    "X_train = torch.tensor(scale_orig.fit_transform(dataset_orig_train.features), dtype=torch.float32)\n",
    "y_train = torch.tensor(dataset_orig_train.labels.ravel(), dtype=torch.float32)\n",
    "\n",
    "\n",
    "X_valid = torch.tensor(scale_orig.transform(dataset_orig_valid.features), dtype=torch.float32)\n",
    "y_valid = torch.tensor(dataset_orig_valid.labels.ravel(), dtype=torch.float32)\n",
    "\n",
    "\n",
    "X_test = torch.tensor(scale_orig.transform(dataset_orig_test.features), dtype=torch.float32)\n",
    "y_test = torch.tensor(dataset_orig_test.labels.ravel(), dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, num_deep=10, hid=32, dropout_p=0.2):\n",
    "        super().__init__()\n",
    "        self.fc0 = nn.Linear(input_size, hid)\n",
    "        self.bn0 = nn.BatchNorm1d(hid)\n",
    "        self.fcs = nn.ModuleList([nn.Linear(hid, hid) for _ in range(num_deep)])\n",
    "        self.bns = nn.ModuleList([nn.BatchNorm1d(hid) for _ in range(num_deep)])\n",
    "        self.out = nn.Linear(hid, 2)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        \n",
    "    def forward(self, t):\n",
    "        t = self.bn0(self.dropout(F.relu(self.fc0(t))))\n",
    "        for bn, fc in zip(self.bns, self.fcs):\n",
    "            t = bn(self.dropout(F.relu(fc(t))))\n",
    "        return torch.sigmoid(self.out(t))\n",
    "    \n",
    "    def trunc_forward(self, t):\n",
    "        t = self.bn0(self.dropout(F.relu(self.fc0(t))))\n",
    "        for bn, fc in zip(self.bns, self.fcs):\n",
    "            t = bn(self.dropout(F.relu(fc(t))))\n",
    "        return t\n",
    "    \n",
    "model = Model(dataset_orig_train.features.shape[1])\n",
    "loss_fn = torch.nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patience = (math.inf, None, 0)\n",
    "patience_limit = 4\n",
    "for epoch in range(201):\n",
    "    model.train()\n",
    "    batch_idxs = torch.split(torch.randperm(X_train.size(0)), 64)\n",
    "    train_loss = 0\n",
    "    for batch in batch_idxs:\n",
    "        X = X_train[batch,:]\n",
    "        y = y_train[batch]\n",
    "        optimizer.zero_grad()\n",
    "        loss = loss_fn(model(X)[:,0], y)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "    if epoch % 10 == 0:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            valid_loss = loss_fn(model(X_valid)[:,0], y_valid)\n",
    "        if valid_loss > patience[0]:\n",
    "            patience = (patience[0], patience[1], patience[2]+1)\n",
    "        else:\n",
    "            patience = (valid_loss, model.state_dict(), 0)\n",
    "        if patience[2] > patience_limit:\n",
    "            print(\"Ending early, patience limit has been crossed without an increase in validation loss!\")\n",
    "            model.load_state_dict(patience[1])\n",
    "            break\n",
    "        print(f'=======> Epoch: {epoch} Train loss: {train_loss / len(batch_idxs)} Valid loss: {valid_loss} Patience valid loss: {patience[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, roc_auc_score, f1_score, classification_report\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_valid_hat = model(X_valid)[:,0]\n",
    "\n",
    "fpr, tpr, thresh = roc_curve(y_valid, y_valid_hat)\n",
    "roc_auc = roc_auc_score(y_valid, y_valid_hat)\n",
    "\n",
    "plt.figure()\n",
    "lw = 2\n",
    "plt.plot(fpr, tpr, color='darkorange',\n",
    "         lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic example')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "x = np.linspace(0,1,1000)\n",
    "fscore = [f1_score(y_valid, y_valid_hat > i) for i in x]\n",
    "plt.plot(x, fscore)\n",
    "plt.show()\n",
    "\n",
    "best_thresh = x[np.argmax(fscore)]\n",
    "print(f'Threshold to maximize f1 score is {best_thresh}')\n",
    "print(classification_report(y_valid, y_valid_hat > best_thresh))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Model (Logistic Regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "# Logistic regression classifier and predictions for training data]\n",
    "lmod = LogisticRegression()\n",
    "lmod.fit(X_train, y_train)\n",
    "\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, f1_score\n",
    "# Prediction probs for validation and testing data\n",
    "X_valid = torch.tensor(scale_orig.transform(dataset_orig_valid.features), dtype=torch.float32)\n",
    "y_valid_hat = lmod.predict_proba(X_valid)[:, 1]\n",
    "y_valid = dataset_orig_valid.labels.ravel()\n",
    "\n",
    "fpr, tpr, thresh = roc_curve(y_valid, y_valid_hat)\n",
    "roc_auc = roc_auc_score(y_valid, y_valid_hat)\n",
    "\n",
    "plt.figure()\n",
    "lw = 2\n",
    "plt.plot(fpr, tpr, color='darkorange',\n",
    "         lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic example')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "x = np.linspace(0,1,1000)\n",
    "fscore = [f1_score(y_valid, y_valid_hat > i) for i in x]\n",
    "plt.plot(x, fscore)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "best_thresh = x[np.argmax(fscore)]\n",
    "print(f'Threshold to maximize f1 score is {best_thresh}')\n",
    "print(classification_report(y_valid, y_valid_hat > best_thresh))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "y_valid_hat = rf.predict_proba(X_valid)[:,1]\n",
    "fpr, tpr, thresh = roc_curve(y_valid, y_valid_hat)\n",
    "roc_auc = roc_auc_score(y_valid, y_valid_hat)\n",
    "\n",
    "plt.figure()\n",
    "lw = 2\n",
    "plt.plot(fpr, tpr, color='darkorange',\n",
    "         lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic example')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "x = np.linspace(0,1,1000)\n",
    "fscore = [f1_score(y_valid, y_valid_hat > i) for i in x]\n",
    "plt.plot(x, fscore)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "best_thresh = x[np.argmax(fscore)]\n",
    "print(f'Threshold to maximize f1 score is {best_thresh}')\n",
    "print(classification_report(y_valid, y_valid_hat > best_thresh))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "from aif360.metrics import ClassificationMetric\n",
    "\n",
    "def compute_metrics(dataset_true, dataset_pred, \n",
    "                    unprivileged_groups, privileged_groups,\n",
    "                    disp = True):\n",
    "    \"\"\" Compute the key metrics \"\"\"\n",
    "    classified_metric_pred = ClassificationMetric(dataset_true,\n",
    "                                                 dataset_pred, \n",
    "                                                 unprivileged_groups=unprivileged_groups,\n",
    "                                                 privileged_groups=privileged_groups)\n",
    "    metrics = OrderedDict()\n",
    "    metrics[\"Balanced accuracy\"] = 0.5*(classified_metric_pred.true_positive_rate()+\n",
    "                                             classified_metric_pred.true_negative_rate())\n",
    "    metrics[\"Statistical parity difference\"] = classified_metric_pred.statistical_parity_difference()\n",
    "    metrics[\"Disparate impact\"] = classified_metric_pred.disparate_impact()\n",
    "    metrics[\"Average odds difference\"] = classified_metric_pred.average_odds_difference()\n",
    "    metrics[\"Equal opportunity difference\"] = classified_metric_pred.equal_opportunity_difference()\n",
    "    metrics[\"Theil index\"] = classified_metric_pred.theil_index()\n",
    "    \n",
    "    if disp:\n",
    "        for k in metrics:\n",
    "            print(\"%s = %.4f\" % (k, metrics[k]))\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    dataset_orig_train_pred = dataset_orig_train.copy(deepcopy=True)\n",
    "    dataset_orig_train_pred.scores = model(X_train)[:,0].reshape(-1,1).numpy()\n",
    "\n",
    "    dataset_orig_valid_pred = dataset_orig_valid.copy(deepcopy=True)\n",
    "    dataset_orig_valid_pred.scores = model(X_valid)[:,0].reshape(-1,1).numpy()\n",
    "\n",
    "    dataset_orig_test_pred = dataset_orig_test.copy(deepcopy=True)\n",
    "    dataset_orig_test_pred.scores = model(X_test)[:,0].reshape(-1,1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the validation set\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    dataset_transf_valid_pred = dataset_orig_valid.copy(deepcopy=True)\n",
    "    dataset_transf_valid_pred.labels = (model(X_valid)[:,0] > best_thresh).reshape(-1,1).numpy()\n",
    "\n",
    "display(Markdown(\"#### Validation set - Initial\"))\n",
    "display(Markdown(\"##### Transformed predictions\"))\n",
    "metric_valid_aft = compute_metrics(dataset_orig_valid, dataset_transf_valid_pred, unprivileged_groups, privileged_groups)\n",
    "\n",
    "display(Markdown(\"##### Classification Report\"))\n",
    "print(classification_report(y_valid, dataset_transf_valid_pred.labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reject Option Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allowed_metrics = [\"Statistical parity difference\", \"Average odds difference\", \"Equal opportunity difference\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aif360.algorithms.postprocessing.reject_option_classification import RejectOptionClassification\n",
    "ROC = RejectOptionClassification(unprivileged_groups=unprivileged_groups, \n",
    "                                 privileged_groups=privileged_groups, \n",
    "                                 low_class_thresh=0.01, high_class_thresh=0.99,\n",
    "                                  num_class_thresh=100, num_ROC_margin=50,\n",
    "                                  metric_name=\"Statistical parity difference\",\n",
    "                                  metric_ub=0.05, metric_lb=-0.05)\n",
    "ROC = ROC.fit(dataset_orig_valid, dataset_orig_valid_pred)\n",
    "\n",
    "roc_thresh = ROC.classification_threshold\n",
    "\n",
    "print(\"Optimal classification threshold (with fairness constraints) = %.4f\" % roc_thresh)\n",
    "print(\"Optimal ROC margin = %.4f\" % ROC.ROC_margin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the validation set\n",
    "dataset_transf_valid_pred = ROC.predict(dataset_orig_valid_pred)\n",
    "\n",
    "display(Markdown(\"#### Validation set - With ROC fairness\"))\n",
    "display(Markdown(\"##### Transformed predictions\"))\n",
    "metric_valid_aft = compute_metrics(dataset_orig_valid, dataset_transf_valid_pred, unprivileged_groups, privileged_groups)\n",
    "\n",
    "display(Markdown(\"##### Classification Report\"))\n",
    "print(classification_report(y_valid, dataset_transf_valid_pred.labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calibrated Equalized Odds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Odds equalizing post-processing algorithm\n",
    "from aif360.algorithms.postprocessing.calibrated_eq_odds_postprocessing import CalibratedEqOddsPostprocessing\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# cost constraint of fnr will optimize generalized false negative rates, that of\n",
    "# fpr will optimize generalized false positive rates, and weighted will optimize\n",
    "# a weighted combination of both\n",
    "cost_constraint = \"fnr\" # \"fnr\", \"fpr\", \"weighted\"\n",
    "randseed=101\n",
    "\n",
    "\n",
    "# Learn parameters to equalize odds and apply to create a new dataset\n",
    "cpp = CalibratedEqOddsPostprocessing(privileged_groups = privileged_groups,\n",
    "                                     unprivileged_groups = unprivileged_groups,\n",
    "                                     cost_constraint=cost_constraint,\n",
    "                                     seed=randseed)\n",
    "cpp = cpp.fit(dataset_orig_valid, dataset_orig_valid_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_transf_valid_pred = cpp.predict(dataset_orig_valid_pred)\n",
    "\n",
    "display(Markdown(\"#### Validation sets - With CalibEqOdds fairness\"))\n",
    "display(Markdown(\"##### Transformed prediction\"))\n",
    "metric_valid_aft = compute_metrics(dataset_orig_valid, dataset_transf_valid_pred, unprivileged_groups, privileged_groups)\n",
    "\n",
    "display(Markdown(\"##### Classification Report\"))\n",
    "print(classification_report(y_valid, dataset_transf_valid_pred.labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from aif360.algorithms.postprocessing.reject_option_classification import RejectOptionClassification\n",
    "# ROC = RejectOptionClassification(unprivileged_groups=unprivileged_groups, \n",
    "#                                  privileged_groups=privileged_groups, \n",
    "#                                  low_class_thresh=0.01, high_class_thresh=0.99,\n",
    "#                                   num_class_thresh=100, num_ROC_margin=50,\n",
    "#                                   metric_name=\"Statistical parity difference\",\n",
    "#                                   metric_ub=0.05, metric_lb=-0.05)\n",
    "\n",
    "results = []\n",
    "for _ in range(50):\n",
    "    rand_model = Model(X_train.size(1))\n",
    "    rand_model.load_state_dict(model.state_dict())\n",
    "    for param in rand_model.parameters():\n",
    "        param.data = param.data * (torch.randn_like(param) + 1)\n",
    "\n",
    "    rand_model.eval()\n",
    "    with torch.no_grad():\n",
    "        dataset_orig_valid_pred = dataset_orig_valid.copy(deepcopy=True)\n",
    "        dataset_orig_valid_pred.scores = rand_model(X_valid)[:,0].reshape(-1,1).numpy()\n",
    "    \n",
    "    roc_auc = roc_auc_score(dataset_orig_valid.labels, dataset_orig_valid_pred.scores)\n",
    "    threshs = np.linspace(0,1,101)\n",
    "    fscores = []\n",
    "    for thresh in threshs:\n",
    "        fscores.append(f1_score(dataset_orig_valid.labels, dataset_orig_valid_pred.scores > thresh))\n",
    "    best_rand_thresh = threshs[np.argmax(fscores)]\n",
    "    dataset_orig_valid_pred.labels = dataset_orig_valid_pred.scores > best_rand_thresh\n",
    "\n",
    "#     ROC = ROC.fit(dataset_orig_valid, dataset_orig_valid_pred)\n",
    "\n",
    "#     dataset_transf_valid_pred = ROC.predict(dataset_orig_valid_pred)\n",
    "    classified_metric_pred = ClassificationMetric(dataset_orig_valid, dataset_orig_valid_pred, unprivileged_groups=unprivileged_groups, privileged_groups=privileged_groups)\n",
    "    spd = classified_metric_pred.statistical_parity_difference()\n",
    "    results.append([roc_auc, spd])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Critic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "    \n",
    "    def __init__(self, sizein, num_deep=3, hid=32):\n",
    "        super().__init__()\n",
    "        self.fc0 = nn.Linear(sizein, hid)\n",
    "        self.fcs = nn.ModuleList([nn.Linear(hid, hid) for _ in range(num_deep)])\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.out = nn.Linear(hid, 1)\n",
    "        \n",
    "    def forward(self, t):\n",
    "        t = t.reshape(1,-1)\n",
    "        t = self.fc0(t)\n",
    "        for fc in self.fcs:\n",
    "            t = F.relu(fc(t))\n",
    "            t = self.dropout(t)\n",
    "        return self.out(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "truncmodel = Model(X_train.size(1))\n",
    "truncmodel.load_state_dict(model.state_dict())\n",
    "critic = Critic(1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trunc_optimizer = optim.Adam(truncmodel.parameters())\n",
    "critic_optimizer = optim.Adam(critic.parameters())\n",
    "critic_loss_fn = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(51):\n",
    "    \n",
    "    for param in critic.parameters():\n",
    "        param.requires_grad = True\n",
    "    for param in truncmodel.parameters():\n",
    "        param.requires_grad = False\n",
    "    truncmodel.eval()\n",
    "    critic.train()\n",
    "    for step in range(201):\n",
    "        critic_optimizer.zero_grad()\n",
    "        indices = torch.randint(0, X_valid.size(0), (32,))\n",
    "        cy_valid = y_valid[indices]\n",
    "        cX_valid = X_valid[indices]\n",
    "        with torch.no_grad():\n",
    "            dataset_orig_valid_pred = dataset_orig_valid.copy(deepcopy=True)\n",
    "            dataset_orig_valid_pred.scores = truncmodel(cX_valid)[:,0].reshape(-1,1).numpy()\n",
    "        \n",
    "        classified_metric_pred = ClassificationMetric(dataset_orig_valid, dataset_orig_valid_pred, unprivileged_groups=unprivileged_groups, privileged_groups=privileged_groups)\n",
    "        spd = classified_metric_pred.statistical_parity_difference()\n",
    "        \n",
    "        res = critic(truncmodel.trunc_forward(cX_valid))\n",
    "        loss = critic_loss_fn(torch.tensor(spd), res[0])\n",
    "        loss.backward()\n",
    "        train_loss = loss.item()\n",
    "        critic_optimizer.step()\n",
    "        if step % 100 == 0:\n",
    "            print(f'=======> Epoch: {(epoch, step)} Critic loss: {train_loss}')\n",
    "            \n",
    "    for param in critic.parameters():\n",
    "        param.requires_grad = False\n",
    "    for param in truncmodel.parameters():\n",
    "        param.requires_grad = True\n",
    "    truncmodel.train()\n",
    "    critic.eval()\n",
    "    for step in range(101):\n",
    "        trunc_optimizer.zero_grad()\n",
    "        indices = torch.randint(0, X_valid.size(0), (32,))\n",
    "        cy_valid = y_valid[indices]\n",
    "        cX_valid = X_valid[indices]\n",
    "        \n",
    "        bias = abs(critic(truncmodel.trunc_forward(cX_valid)))\n",
    "        loss = critic_loss_fn(cy_valid, truncmodel(cX_valid)[:,0])\n",
    "        loss = loss + 2*bias\n",
    "        \n",
    "        loss.backward()\n",
    "        train_loss = loss.item()\n",
    "        trunc_optimizer.step()\n",
    "        if step % 100 == 0:\n",
    "            print(f'=======> Epoch: {(epoch, step)} Actor loss: {train_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    dataset_orig_valid_pred = dataset_orig_valid.copy(deepcopy=True)\n",
    "    dataset_orig_valid_pred.scores = truncmodel(X_valid)[:,0].reshape(-1,1).numpy()\n",
    "\n",
    "roc_auc = roc_auc_score(dataset_orig_valid.labels, dataset_orig_valid_pred.scores)\n",
    "threshs = np.linspace(0,1,1001)\n",
    "fscores = []\n",
    "for thresh in threshs:\n",
    "    fscores.append(f1_score(dataset_orig_valid.labels, dataset_orig_valid_pred.scores > thresh))\n",
    "best_rand_thresh = threshs[np.argmax(fscores)]\n",
    "dataset_orig_valid_pred.labels = dataset_orig_valid_pred.scores > best_rand_thresh\n",
    "\n",
    "display(Markdown(\"#### Validation sets - With Critic fairness\"))\n",
    "display(Markdown(\"##### Transformed prediction\"))\n",
    "metric_valid_aft = compute_metrics(dataset_orig_valid, dataset_orig_valid_pred, unprivileged_groups, privileged_groups)\n",
    "\n",
    "display(Markdown(\"##### Classification Report\"))\n",
    "print(classification_report(y_valid, dataset_transf_valid_pred.labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aif360.algorithms.postprocessing.reject_option_classification import RejectOptionClassification\n",
    "ROC = RejectOptionClassification(unprivileged_groups=unprivileged_groups, \n",
    "                                 privileged_groups=privileged_groups, \n",
    "                                 low_class_thresh=0.01, high_class_thresh=0.99,\n",
    "                                  num_class_thresh=100, num_ROC_margin=50,\n",
    "                                  metric_name=\"Statistical parity difference\",\n",
    "                                  metric_ub=0.05, metric_lb=-0.05)\n",
    "with torch.no_grad():\n",
    "    dataset_orig_valid_pred = dataset_orig_valid.copy(deepcopy=True)\n",
    "    dataset_orig_valid_pred.scores = truncmodel(X_valid)[:,0].reshape(-1,1).numpy()\n",
    "    \n",
    "\n",
    "ROC = ROC.fit(dataset_orig_valid, dataset_orig_valid_pred)\n",
    "roc_thresh = ROC.classification_threshold\n",
    "\n",
    "# Transform the validation set\n",
    "dataset_transf_valid_pred = ROC.predict(dataset_orig_valid_pred)\n",
    "\n",
    "display(Markdown(\"#### Validation set - With Critic + ROC fairness\"))\n",
    "display(Markdown(\"##### Transformed predictions\"))\n",
    "metric_valid_aft = compute_metrics(dataset_orig_valid, dataset_transf_valid_pred, unprivileged_groups, privileged_groups)\n",
    "\n",
    "display(Markdown(\"##### Classification Report\"))\n",
    "print(classification_report(y_valid, dataset_transf_valid_pred.labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deco",
   "language": "python",
   "name": "deco"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
